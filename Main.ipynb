{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdoZano/AMD/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark==3.5.1\n",
        "!pip install -q spark-nlp==3.4.3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcIqi0JrOuag",
        "outputId": "662b99d1-240d-4d13-e6a7-3eddd352d548"
      },
      "id": "jcIqi0JrOuag",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/53.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/144.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.6/144.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "25f156c5",
      "metadata": {
        "id": "25f156c5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col, size, expr, avg, abs #expr allows you to write SQL inside pyspark\n",
        "from pyspark.ml.feature import HashingTF, MinHashLSH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c74403d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c74403d1",
        "outputId": "851d8ef7-619e-4f83-a7af-2a3aeda195c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content\n",
            " 95% 1.01G/1.06G [00:06<00:00, 94.2MB/s]\n",
            "100% 1.06G/1.06G [00:07<00:00, 161MB/s] \n",
            "Archive:  amazon-books-reviews.zip\n",
            "  inflating: Books_rating.csv        \n",
            "  inflating: books_data.csv          \n"
          ]
        }
      ],
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"XXXX\"  # <-- your username\n",
        "os.environ['KAGGLE_KEY'] = \"XXXX\"       # <-- your key API\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews\n",
        "\n",
        "!unzip amazon-books-reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df = pd.read_csv(\"Books_rating.csv\")\n",
        "print(len(ratings_df))\n",
        "\n",
        "print(f'{ratings_df.columns}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TR952GaQYjf",
        "outputId": "1c2f5750-914f-4be1-cd48-8896063ccf83"
      },
      "id": "7TR952GaQYjf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000000\n",
            "Index(['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness',\n",
            "       'review/score', 'review/time', 'review/summary', 'review/text'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0a40e8d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a40e8d2",
        "outputId": "956a38d8-9b81-495a-8030-b7c3cdfb3ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series([], Name: Title, dtype: int64)\n",
            "2970140\n",
            "2062648\n"
          ]
        }
      ],
      "source": [
        "# Check if the same 'Id' (book identifier) has multiple different 'Title' entries\n",
        "inconsistent_id_title = ratings_df.groupby(\"Id\")[\"Title\"].nunique()\n",
        "print(inconsistent_id_title[inconsistent_id_title > 1])  # Show only inconsistent cases\n",
        "\n",
        "# Remove rows with missing review text, keep only these columns\n",
        "ratings_df = ratings_df[[\"Id\", \"Title\", \"User_id\", \"review/text\"]].dropna(subset=[\"review/text\"])\n",
        "\n",
        "# Drop completely identical rows (same user, book, text, etc.)\n",
        "ratings_df = ratings_df.drop_duplicates()\n",
        "print(f'After first cleanins we have : {len(ratings_df)} datapoints')\n",
        "# Remove reviews with the same exact text\n",
        "ratings_df = ratings_df.drop_duplicates(subset=[\"review/text\"])\n",
        "print(f'At the end we have : {len(ratings_df)} datapoints')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "995f1209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "995f1209",
        "outputId": "35fa160e-5663-45c8-ba80-6ade961e05b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68.75493333333333"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "print(f'Percentage of the total dataset kept: {len(ratings_df)/30000}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5ac97e6a",
      "metadata": {
        "id": "5ac97e6a"
      },
      "outputs": [],
      "source": [
        "USE_SAMPLING = True\n",
        "if USE_SAMPLING:\n",
        "    sample_df = ratings_df.sample(frac=0.05, random_state=42)\n",
        "else:\n",
        "    sample_df = ratings_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eba15ec3",
      "metadata": {
        "id": "eba15ec3"
      },
      "outputs": [],
      "source": [
        "# ğŸ”¹ Start Spark NLP session\n",
        "spark = sparknlp.start()\n",
        "\n",
        "# ğŸ”¹ Create Spark DataFrame\n",
        "spark_df = spark.createDataFrame(sample_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "91433545",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91433545",
        "outputId": "9ab933b0-7aba-4caa-80fa-b07ce4b460a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+--------------+--------------------+\n",
            "|        Id|               Title|       User_id|         review/text|\n",
            "+----------+--------------------+--------------+--------------------+\n",
            "|0201379619|Dynamic HTML: The...|           NaN|I like it. It's v...|\n",
            "|B0000A9AYK|       Dry: A Memoir|A2KEH545A8283S|This book has it ...|\n",
            "|1590481208|Hidalgo and Other...| AOL4M7FJMQC6Q|After watching th...|\n",
            "|B000J58B8Y|FIFTH ELEPHANT (D...|A2EMBVAUBHA5WI|I admit that this...|\n",
            "|0761125493|What to Expect Wh...|           NaN|I found that even...|\n",
            "+----------+--------------------+--------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark_df.show(5)\n",
        "spark_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "43acd860",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43acd860",
        "outputId": "e7ee47a9-1da8-463d-fa08-e1166d8a773b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Id        |shingles                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0201379619|[like interesting way, interesting way romeo, way romeo juliet, romeo juliet montague, juliet montague capulet, montague capulet somewhat, capulet somewhat clandestine, somewhat clandestine relationship, clandestine relationship eachother, relationship eachother parted, eachother parted way, parted way juliet, way juliet romeo, juliet romeo die, romeo die end, die end made, end made realize, made realize strong, realize strong love, strong love paramours]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|B0000A9AYK|[book drama humor, drama humor suspense, humor suspense couldnt, suspense couldnt put]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "|1590481208|[watching movie husband, movie husband wanted, husband wanted know, wanted know hopkins, know hopkins movie, hopkins movie said, movie said based, said based actual, based actual events, actual events thought, events thought book, thought book fun, book fun read, fun read ever, read ever read, ever read hopkins, read hopkins liked, hopkins liked tell, liked tell untrue, tell untrue stories, untrue stories wasnt, stories wasnt interested, wasnt interested reading, interested reading anymore, reading anymore kind, anymore kind spoiled, kind spoiled movie, spoiled movie also, movie also wish, also wish people, wish people made, people made movie, made movie done, movie done little, done little reasurch, little reasurch said, reasurch said actually, said actually happened]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|B000J58B8Y|[admit one book, one book probably, book probably doesnt, probably doesnt need, doesnt need another, need another review, another review praise, review praise hard, praise hard recommend, hard recommend th, recommend th elephant, th elephant carry, elephant carry around, carry around esp, around esp hardcover, esp hardcover edition, hardcover edition youll, edition youll get, youll get strange, get strange looks, strange looks people, looks people wondering, people wondering kind, wondering kind weird, kind weird book, weird book youre, book youre reading, youre reading saying, reading saying quotisnt, saying quotisnt fifth, quotisnt fifth element, fifth element bruce, element bruce willisquotbut, bruce willisquotbut nothing, willisquotbut nothing farther, nothing farther truth, farther truth colorful, truth colorful characters, colorful characters nonstop, characters nonstop action, nonstop action zany, action zany settings, zany settings pratchett, settings pratchett made, pratchett made ankhmorpork, made ankhmorpork discworld, ankhmorpork discworld wonderful, discworld wonderful place, wonderful place get, place get away, get away sometimes, away sometimes british, sometimes british humor, british humor little, humor little odd, little odd american, odd american readers, american readers get, readers get used, get used nevertheless, used nevertheless always, nevertheless always amusing, always amusing wonderful, amusing wonderful way, wonderful way get, way get know, get know wonderful, know wonderful fantasy, wonderful fantasy series]|\n",
            "|0761125493|[found even though, even though book, though book well, book well organized, well organized easy, organized easy read, easy read provide, read provide risks, provide risks side, risks side affects, side affects natural, affects natural alternatives, natural alternatives many, alternatives many medical, many medical procedures, medical procedures presented, procedures presented encourages, presented encourages kind, encourages kind wait, kind wait see, wait see attitude, see attitude terms, attitude terms preparing, terms preparing labor, preparing labor birth, labor birth instead, birth instead really, instead really providing, really providing information, providing information factual, information factual way, factual way women, way women think, women think make, think make best, make best informed, best informed decisions, informed decisions regarding, decisions regarding procedures, regarding procedures meds, procedures meds found, meds found somewhat, found somewhat irresponsible]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”¹ Document assembler\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"review/text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# ğŸ”¹ Tokenizer\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# ğŸ”¹ Normalizer: lowercase + punctuation removed\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\") \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "# ğŸ”¹ Stopword remover\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCol(\"clean_tokens\") \\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "# ğŸ”¹ Finisher\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"clean_tokens\"]) \\\n",
        "    .setOutputCols([\"final_tokens\"]) \\\n",
        "    .setCleanAnnotations(True)\n",
        "\n",
        "# ğŸ”¹ Pipeline NLP\n",
        "nlp_pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    normalizer,\n",
        "    stopwords_cleaner,\n",
        "    finisher\n",
        "])\n",
        "\n",
        "# ğŸ”¹ Fit & Transform\n",
        "model = nlp_pipeline.fit(spark_df)\n",
        "processed_df = model.transform(spark_df)\n",
        "\n",
        "# ğŸ”¹ Keep rows with at least 3 token (for shingles)\n",
        "filtered_df = processed_df.withColumn(\"num_tokens\", size(col(\"final_tokens\"))) \\\n",
        "    .filter(col(\"num_tokens\") >= 3)\n",
        "\n",
        "# ğŸ”¹ Create shingles (k=3)\n",
        "shingled_df = filtered_df.withColumn(\n",
        "    \"shingles\",\n",
        "    expr(\"transform(sequence(0, size(final_tokens)-3), i -> concat_ws(' ', final_tokens[i], final_tokens[i+1], final_tokens[i+2]))\")\n",
        ")\n",
        "#concat_ws(' ', ...) Unisce le tre parole in una stringa unica con spazio:\n",
        "\n",
        "\n",
        "# ğŸ”¹ Show final dataframe\n",
        "shingled_df.select(\"Id\", \"shingles\").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2286853a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2286853a",
        "outputId": "5946be0a-f0b9-47ee-c028-6c6c9f93d6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-------------------+\n",
            "|ID_A      |ReviewA_Preview                                                                                                                                                                                         |n_tokens_A|ID_B      |ReviewB_Preview                                                                                                                                                                                         |n_tokens_B|JaccardDistance    |\n",
            "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-------------------+\n",
            "|0967799503|After all the years of digging through the mud of the Matrix Control System and collecting data which leads to the inescapable idea that we are not at the top of the Food Chain here on the Big Blue Ma|2602      |B000J4JWUQ|Dawkins warns his readers that his language, where it constantly may appear to hold `subjective' or `moral' or teleological content, does not really--in fact it is argued to hold no real connotations |1196      |0.119140625        |\n",
            "|0967799503|After all the years of digging through the mud of the Matrix Control System and collecting data which leads to the inescapable idea that we are not at the top of the Food Chain here on the Big Blue Ma|2602      |1566636736|I would really like to believe what Ben Wattenberg writes in this book \"Fewer\" about the prospects for world population. He thinks that overpopulation is no threat at all. Instead, he says that depopu|1653      |0.05078125         |\n",
            "|0967799503|After all the years of digging through the mud of the Matrix Control System and collecting data which leads to the inescapable idea that we are not at the top of the Food Chain here on the Big Blue Ma|2602      |B000N6R4AA|Caution: move slowly through this review and give the whole message a chance, especially if you are a die hard Eckhart Tolle fan. I am still a big fan, but this time, there is something amiss and I'm |997       |0.15068493150684936|\n",
            "|0941532631|In this book which includes essays written over almost half a century, Deborah Weiss-Duthilh introduces the reader to a French traditionalist author so far largely unknown in the English-speaking worl|1284      |0967799503|After all the years of digging through the mud of the Matrix Control System and collecting data which leads to the inescapable idea that we are not at the top of the Food Chain here on the Big Blue Ma|2602      |0.111328125        |\n",
            "|1566636736|I would really like to believe what Ben Wattenberg writes in this book \"Fewer\" about the prospects for world population. He thinks that overpopulation is no threat at all. Instead, he says that depopu|1653      |B000J4JWUQ|Dawkins warns his readers that his language, where it constantly may appear to hold `subjective' or `moral' or teleological content, does not really--in fact it is argued to hold no real connotations |1196      |0.14677103718199613|\n",
            "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\n",
        "# ğŸ”¹ Apply HashingTF to transform shingles into vectors\n",
        "hashingTF = HashingTF(inputCol=\"shingles\", outputCol=\"features\", numFeatures=512)\n",
        "featurized_df = hashingTF.transform(shingled_df)\n",
        "\n",
        "# ğŸ”¹ Build MinHashLSH model\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=3)\n",
        "lsh_model = mh.fit(featurized_df)\n",
        "mh_df = lsh_model.transform(featurized_df).cache()\n",
        "\n",
        "# ğŸ”¹ Find reviews pair close to each others (Jaccard approximated)\n",
        "similar_pairs = lsh_model.approxSimilarityJoin(\n",
        "    mh_df,\n",
        "    mh_df,\n",
        "    threshold=0.2,  # puoi regolare\n",
        "    distCol=\"JaccardDistance\"\n",
        ")\n",
        "\n",
        "# ğŸ”¹ Don't count same IDs\n",
        "similar_pairs_filtered = similar_pairs.filter(col(\"datasetA.Id\") < col(\"datasetB.Id\"))\n",
        "\n",
        "filtered_output = similar_pairs_filtered \\\n",
        "    .select(\n",
        "        col(\"datasetA.Id\").alias(\"ID_A\"),\n",
        "        expr(\"substring(datasetA.`review/text`, 1, 200)\").alias(\"ReviewA_Preview\"),\n",
        "        col(\"datasetA.num_tokens\").alias(\"n_tokens_A\"),\n",
        "\n",
        "        col(\"datasetB.Id\").alias(\"ID_B\"),\n",
        "        expr(\"substring(datasetB.`review/text`, 1, 200)\").alias(\"ReviewB_Preview\"),\n",
        "        col(\"datasetB.num_tokens\").alias(\"n_tokens_B\"),\n",
        "\n",
        "        col(\"JaccardDistance\")\n",
        "    )\n",
        "\n",
        "\n",
        "# ğŸ”¹ Show similar pairs\n",
        "filtered_output = filtered_output.cache()\n",
        "filtered_output.show(5, truncate=False)\n",
        "filtered_output.count()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4220a9af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4220a9af",
        "outputId": "3145007c-180c-4443-edea-a10f1eb4e7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+------------------+------------------+-----------------+\n",
            "|summary|    JaccardDistance|        n_tokens_A|        n_tokens_B|       token_diff|\n",
            "+-------+-------------------+------------------+------------------+-----------------+\n",
            "|  count|                 61|                61|                61|               61|\n",
            "|   mean| 0.1371365473505715|            1478.0|1598.5409836065573|660.4426229508197|\n",
            "| stddev|0.04285744364225551|488.35871378867944| 554.7056749235127|506.5517586120943|\n",
            "|    min|        0.029296875|               249|               261|               12|\n",
            "|    max|0.19842829076620827|              2602|              2602|             1786|\n",
            "+-------+-------------------+------------------+------------------+-----------------+\n",
            "\n",
            "Correlation JaccardDistance vs token_diff: 0.0068\n",
            "+-----------------+\n",
            "|  avg(token_diff)|\n",
            "+-----------------+\n",
            "|660.4426229508197|\n",
            "+-----------------+\n",
            "\n",
            "+--------------------+\n",
            "|avg(JaccardDistance)|\n",
            "+--------------------+\n",
            "|  0.1371365473505715|\n",
            "+--------------------+\n",
            "\n",
            "+----------+----------+--------------------+----------+----------+----------+\n",
            "|ID_A      |ID_B      |JaccardDistance     |token_diff|n_tokens_A|n_tokens_B|\n",
            "+----------+----------+--------------------+----------+----------+----------+\n",
            "|0967799503|1932100385|0.029296875         |486       |2602      |2116      |\n",
            "|0786229055|B000K0DB8I|0.045454545454545414|12        |249       |261       |\n",
            "|0967799503|1566636736|0.05078125          |949       |2602      |1653      |\n",
            "|0967799503|B0007HS8NM|0.05859375          |930       |2602      |1672      |\n",
            "|1566636736|1932100385|0.060546875         |463       |1653      |2116      |\n",
            "|1932100385|B0007HS8NM|0.068359375         |444       |2116      |1672      |\n",
            "|0030353734|0967799503|0.07045009784735812 |1210      |1392      |2602      |\n",
            "|0030353734|1932100385|0.083984375         |724       |1392      |2116      |\n",
            "|0595330304|0967799503|0.0859375           |1240      |1362      |2602      |\n",
            "|1566636736|B0007HS8NM|0.08610567514677103 |19        |1653      |1672      |\n",
            "+----------+----------+--------------------+----------+----------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------+----------+-------------------+----------+----------+----------+\n",
            "|ID_A      |ID_B      |JaccardDistance    |token_diff|n_tokens_A|n_tokens_B|\n",
            "+----------+----------+-------------------+----------+----------+----------+\n",
            "|0595330304|0967799503|0.0859375          |1240      |1362      |2602      |\n",
            "|0030353734|0967799503|0.07045009784735812|1210      |1392      |2602      |\n",
            "|0967799503|1566636736|0.05078125         |949       |2602      |1653      |\n",
            "|0967799503|B0007HS8NM|0.05859375         |930       |2602      |1672      |\n",
            "|0595330304|1932100385|0.095703125        |754       |1362      |2116      |\n",
            "|0030353734|1932100385|0.083984375        |724       |1392      |2116      |\n",
            "|0967799503|1932100385|0.029296875        |486       |2602      |2116      |\n",
            "|1566636736|1932100385|0.060546875        |463       |1653      |2116      |\n",
            "|1932100385|B0007HS8NM|0.068359375        |444       |2116      |1672      |\n",
            "+----------+----------+-------------------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Calculate difference in token numbers\n",
        "filtered_output = filtered_output.withColumn(\n",
        "    \"token_diff\",\n",
        "    abs(col(\"n_tokens_A\") - col(\"n_tokens_B\"))\n",
        ")\n",
        "#Show analysis for each column\n",
        "filtered_output.select(\"JaccardDistance\",\"n_tokens_A\", \"n_tokens_B\", \"token_diff\").describe().show()\n",
        "#Correlation between variables\n",
        "corr_diff = filtered_output.stat.corr(\"JaccardDistance\", \"token_diff\")\n",
        "\n",
        "print(f\"Correlation JaccardDistance vs token_diff: {corr_diff:.4f}\")\n",
        "#Mean of token difference\n",
        "filtered_output.select(avg(col(\"token_diff\"))).show()\n",
        "#Mean of Jaccard Distance\n",
        "filtered_output.select(avg(col(\"JaccardDistance\"))).show()\n",
        "#Show in ascending order of Jaccard similarity all the pairs\n",
        "filtered_output.orderBy(col(\"JaccardDistance\").asc()).select(\n",
        "    \"ID_A\", \"ID_B\", \"JaccardDistance\", \"token_diff\", \"n_tokens_A\", \"n_tokens_B\"\n",
        ").show(10, truncate=False)\n",
        "\n",
        "#Show pairs with small Jaccard and big token difference\n",
        "filtered_output \\\n",
        "    .filter((col(\"JaccardDistance\") < 0.1) & (col(\"token_diff\") > 30)) \\\n",
        "    .select(\"ID_A\", \"ID_B\", \"JaccardDistance\", \"token_diff\", \"n_tokens_A\", \"n_tokens_B\") \\\n",
        "    .orderBy(col(\"token_diff\").desc()) \\\n",
        "    .show(10, truncate=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "MassiveAlgProject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}